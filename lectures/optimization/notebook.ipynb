{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'temfpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40026385d55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtemfpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcarlberg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'temfpy'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from temfpy.optimization import carlberg\n",
    "\n",
    "from scipy import optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from optimization_problems import get_test_function_gradient\n",
    "from optimization_problems import get_parameterization\n",
    "from optimization_problems import get_test_function\n",
    "from optimization_auxiliary import process_results\n",
    "from optimization_auxiliary import get_bounds\n",
    "from optimization_plots import plot_contour\n",
    "from optimization_plots import plot_surf\n",
    "from optimization_plots import plot_optima_example\n",
    "from optimization_plots import plot_true_observed_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Setup\n",
    "2. Algorithms\n",
    "3. Gradient-based methods\n",
    "4. Derivative-free methods\n",
    "5. Benchmarking exercise\n",
    "6. Special cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the finite-dimensional unconstrained optimization problem, one is given a function $f : R^n \\mapsto R$ and asked to find an $x^\\ast$ such that $f (x^\\ast) \\geq f(x)$ for all $x$. We call $f$ the objective function and $x^\\ast$ , if it exists, the global minimum of $f$. We focus on minimum - to solve a minimization problem, simply minimize the negative of the objective.\n",
    "\n",
    "We say that $x^\\ast \\in R^n$ is a ...\n",
    "\n",
    "* strict global minimum of $f$ if $f(x^\\ast) > f (x)$ for all $x\\neq x^\\ast$.\n",
    "* weak local minimum of $f$ if $f(x^\\ast) \\geq f(x)$ for all $x$ in some neighborhood of $x^\\ast$.\n",
    "* strict local minimum of $f$ if $f(x^\\ast) > f(x)$ for all $x\\neq x^\\ast$ in some neighborhood of $x^\\ast$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_optima_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1377f83e03e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_optima_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_optima_example' is not defined"
     ]
    }
   ],
   "source": [
    "plot_optima_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f: R^n \\mapsto R$ be twice continuously differentiable.\n",
    "\n",
    "* **First Order Necessary Conditions:** If $x^\\ast$ is a local minimum of $f$, then $f^{\\prime}(x^\\ast) = 0$. \n",
    "* **Second Order Necessary Condition:** If $x^\\ast$ is a local minimum of $f$, then $f^{\\prime\\prime}(x^*)$ is negative semidefinite. \n",
    "\n",
    "We say $x$ is a critical point of $f$ if it satisfies the first-order necessary condition.\n",
    "\n",
    "* **Sufficient Condition:** If $f^\\prime (x^\\ast) = 0$ and $f^{\\prime\\prime}(x^\\ast)$ is negative definite, then $x^\\ast$ is a strict local minimum of $f$.\n",
    "* **Local-Global Theorem:** If $f$ is concave, and $x^\\ast$ is a local minimum of $f$, then $x^\\ast$ is a global minimum of $f$.\n",
    "\n",
    "**Key problem attributes**\n",
    "\n",
    "* Convexity: convex vs. non-convex\n",
    "* Optimization-variable type: continuous vs. discrete\n",
    "* Constraints: unconstraint vs. constraint\n",
    "* Number of optimization variables: low-dimensional vs. high-dimensional\n",
    "\n",
    "These attributes dictate:\n",
    "\n",
    "* ability to find solution\n",
    "* problem complexity and computing time\n",
    "* appropriate methods\n",
    "* relevant software\n",
    "\n",
    "$\\Rightarrow$ Always begin by categorizing your problem\n",
    "\n",
    "\n",
    "\n",
    "Optimization problems are ubiquitous in economics:\n",
    "\n",
    "* Government maximizes social welfare\n",
    "* Competitive equilibrium maximizes total surplus\n",
    "* Ordinary least squares estimator minimizes sum of squares\n",
    "* Maximum likelihood estimator maximizes likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Algorithms\n",
    "\n",
    "\n",
    "We are mostly blind to the function we are trying to minimize and can only compute the function at a limited number of points. Each evaluation is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_observed_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "* reasonable memory requirements\n",
    "* low failure rate, convergence conditions are met\n",
    "* convergence in a few iterations with low cost for each iteration\n",
    "\n",
    "**Catergorization**\n",
    "\n",
    "* gradient-based vs. derivative-free\n",
    "* global vs. local\n",
    "\n",
    "\n",
    "### _Question_  \n",
    "\n",
    "* How to compute derivatives?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-based methods\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "* efficient for many variables\n",
    "* well-suited for smooth objective and constraint functions\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "* requires computing the gradient, potentially challenging and time-consuming\n",
    "* convergence is only local\n",
    "* not-well suited for noisy functions, derivative information flawed\n",
    "\n",
    "Second derivative are also very useful, but ...\n",
    "\n",
    "* Hessians are $n\\times n$, so expensive to construct and store\n",
    "* often only approximated using quasi-Newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Questions_\n",
    "\n",
    "1. How to use gradient-based algorithms to find a global optimum?\n",
    "2. Any ideas on how to reduce the memory requirements for a large Hessian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"material/fig-gradient-based-algorithm.png\" width=500 height=500 />\n",
    "\n",
    "\n",
    "<img src=\"material/fig-gradient-based-overview.png\" width=500 height=500 />\n",
    "\n",
    "\n",
    "There is two different classes of gradient-based algorithms.\n",
    "\n",
    "* Line-search methods\n",
    "    * compute $p_k$ be a descent direction\n",
    "    * compute $a_k$ to produce a sufficient decrease in the objective function\n",
    "\n",
    "Let's see [here](https://github.com/scipy/scipy/blob/8e30f7797bd1ee442f4f1a25172e4402521c1e16/scipy/optimize/optimize.py#L1484) for how such a line search looks like in practice for the [Newton-CG](https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method) algorithm.\n",
    "\n",
    "* Trust-region methods\n",
    "    * determine a maximum allowable step length (trust-region radius) $\\delta k$\n",
    "    * compute step $k$ with $||p_k|| \\leq \\Delta$ using a model $m(p) \\approx f(x_k + p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example implementation, see [here](https://github.com/scipy/scipy/blob/46b359bfb54072dae61882731bb2e766e38ba393/scipy/optimize/_trustregion.py#L100) for the `scipy.optimize_trustregion.py` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Derivative-Free Methods\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "* often better at finding a global minimum if function not convex\n",
    "* robust with respect to noise in criterion function\n",
    "* amenable to parallelization\n",
    "\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "* extremely slow convergence for high-dimensional problems\n",
    "\n",
    "There are two different classes of derivative-free algorithms.\n",
    "\n",
    "* heuristic, inspired by nature\n",
    "    * basin-hopping\n",
    "    * evolutionary algorithms\n",
    "\n",
    "* direct search\n",
    "    * directional\n",
    "    * simplicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\tfrac{1}{2}\\sum_{i=1}^n a_i\\cdot (x_i-1)^2+ b\\cdot \\left[ n-\\sum_{i=1}^n\\cos(2\\pi(x_i-1))\\right],\n",
    "$$\n",
    "\n",
    "where $a_i$ and $b$ provide the parameterization of the function.\n",
    "\n",
    "\n",
    "### _Exercises_\n",
    "\n",
    "1. Implement this test function.\n",
    "2. Visualize the shape of our test function for the one-dimensional case.\n",
    "3. What is the role of the parameters $a_1$ and $b$?\n",
    "4. What is the functions global minimum? \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_test_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to use our test function for different configurations of the challenges introduced by noise and ill-conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise, add_illco, x0 = False, False, [4.5, -1.5]\n",
    "\n",
    "\n",
    "def get_problem(dimension, add_noise, add_illco, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    a, b = get_parameterization(dimension, add_noise, add_illco)\n",
    "    get_test_function_p = partial(get_test_function, a=a, b=b)\n",
    "    get_test_function_gradient_p = partial(get_test_function_gradient, a=a, b=b)\n",
    "    return get_test_function_p, get_test_function_gradient_p\n",
    "\n",
    "\n",
    "dimension = len(x0)\n",
    "opt_test_function, opt_test_gradient = get_problem(dimension, add_noise, add_illco)\n",
    "np.testing.assert_equal(opt_test_function([1, 1]), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the surface and contour plots look like under different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_test_function, _ = get_problem(dimension, add_noise, add_illco)\n",
    "plot_surf(opt_test_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Question_\n",
    "\n",
    "* How is the global minimum affected by the addition of noise and ill-conditioning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Benchmarking exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our problem setting and initialize a container for our results. We will use the convenient interface to [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize). Its documentation also points you to research papers and textbooks where the details of the algorithms are discussed in more detail. We need to invest a little in the design of our setup first, but then we can run the benchmarking exercise with ease and even adding additional optimization algorithms is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = [\"CG\", \"Newton-CG\", \"Nelder-Mead\", \"Diff-Evol\"]\n",
    "add_noise, add_illco, dimension = False, False, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [4.5, -1.5]\n",
    "opt_test_function, opt_test_gradient = get_problem(dimension, add_noise, add_illco)\n",
    "df = pd.DataFrame(columns=[\"Iteration\", \"Distance\"], index=ALGORITHMS)\n",
    "df.index.name = \"Method\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix what will stay unchanged throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_optimizer = partial(\n",
    "    opt.minimize,\n",
    "    fun=opt_test_function,\n",
    "    x0=x0,\n",
    "    jac=opt_test_gradient,\n",
    "    options={\"disp\": True, \"return_all\": True, \"maxiter\": 100000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepared some functions to process results from the optimizer calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??process_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"CG\"\n",
    "res = call_optimizer(method=method)\n",
    "df = process_results(df, method, res)\n",
    "plot_contour(opt_test_function, res[\"allvecs\"], method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Newton-CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"Newton-CG\"\n",
    "res = call_optimizer(method=method)\n",
    "df = process_results(df, method, res)\n",
    "plot_contour(opt_test_function, res[\"allvecs\"], method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nelder Mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"Nelder-Mead\"\n",
    "res = call_optimizer(method=method)\n",
    "df = process_results(df, method, res)\n",
    "plot_contour(opt_test_function, res[\"allvecs\"], method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"Diff-Evol\"\n",
    "res = opt.differential_evolution(opt_test_function, get_bounds(dimension))\n",
    "plot_contour(opt_test_function, res[\"x\"], method)\n",
    "df = process_results(df, method, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.barplot(x=\"Method\", y=\"Iteration\", data=df.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.barplot(x=\"Method\", y=\"Distance\", data=df.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up test function\n",
    "\n",
    "We want to increase the dimensionality of our optimization problem going forward. Even in this easy setting, it is worth to re-write our objective function using `numpy` to ensure its speedy execution. A faster version is already available as part of the Python package temfpy. Below, we compare our test function to the [temfpy version](https://temfpy.readthedocs.io/en/latest/optimization.html#temfpy.optimization.carlberg) and assess their performance in regard to speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_test_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??carlberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to introduce errors when speeding up your code as usually you face a trade-off between readability and performance. However, setting up a simple testing harness that simply compares the results between the slow, but readable, implementation and the fast one for numerous random test problems. For more automated, but random, testing see [Hypothesis](https://hypothesis.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed_test_problem():\n",
    "    add_illco, add_noise = np.random.choice([True, False], size=2)\n",
    "    dimension = np.random.randint(2, 100)\n",
    "\n",
    "    a, b = get_parameterization(dimension, add_noise, add_illco)\n",
    "    x0 = np.random.uniform(size=dimension)\n",
    "    return x0, a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to put our fears at ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    args = get_speed_test_problem()\n",
    "    stats = get_test_function(*args), carlberg(*args)\n",
    "    np.testing.assert_almost_equal(*stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether this was worth the effort for a small and a large problem using the `%timeit` magic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension, add_noise, add_illco = 100, True, True\n",
    "x0 = np.random.uniform(size=dimension)\n",
    "a, b = get_parameterization(dimension, add_noise, add_illco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit carlberg(x0, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_test_function(x0, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular setting, there is no need to increase the performance even further. However, as a next step, check out [numba](https://numba.pydata.org/), for even more flexibility in speeding up your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercises_\n",
    "\n",
    "1. Repeat the exercise in the case of noise in the criterion function and try to summarize your findings. \n",
    "2. What additional problems arise as the dimensionality of the problem for a 100-dimensional problem? Make sure to use the fast implementation of the test function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Special cases\n",
    "        \n",
    "\n",
    "Nonlinear least squares and maximum likelihood estimation have special structure that can be exploited to improve the approximation of the inverse Hessian.\n",
    "\n",
    "\n",
    "### Nonlinear least squares\n",
    "\n",
    "We will estimate the following nonlinear consumption function using data from Greene's textbook:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "C = \\alpha  + \\beta \\times Y^\\gamma + \\epsilon \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which is estimated with quarterly data on real consumption and disposable income for the U.S. economy from 1950 to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"material/data-consumption-function.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the basic relationship to get an idea of what to expect for the estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = df.index.get_level_values(\"Year\")\n",
    "\n",
    "for name in [\"realgdp\", \"realcons\"]:\n",
    "    y = df[name]\n",
    "    ax.plot(x, y, label=name)\n",
    "\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the criterion function such that it fits the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption = df[\"realcons\"].values\n",
    "income = df[\"realgdp\"].values\n",
    "\n",
    "\n",
    "def ssr(x, consumption, income):\n",
    "    alpha, beta, gamma = x\n",
    "    residuals = consumption - alpha - beta * income ** gamma\n",
    "    return residuals\n",
    "\n",
    "\n",
    "ssr_partial = partial(ssr, consumption=consumption, income=income)\n",
    "rslt = sp.optimize.least_squares(ssr_partial, [0, 0, 1])[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_\n",
    "\n",
    "* Evaluate the fit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation\n",
    "\n",
    "Greene (2012) considers the following binary choice model.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P[Grade = 1] = F(\\beta_0 + \\beta_1 GPA + \\beta_2 TUCE + \\beta_3 PSI)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $F$ the cumulative distribution function for either the normal distribution (Probit) or the logistic distribution (Logit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"material/data-graduation-prediction.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probit_model(beta, y, x):\n",
    "    F = norm.cdf(x @ beta)\n",
    "    fval = (y * np.log(F) + (1 - y) * np.log(1 - F)).sum()\n",
    "    return -fval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df[[\"INTERCEPT\", \"GPA\", \"TUCE\", \"PSI\"]], df[\"GRADE\"]\n",
    "rslt = opt.minimize(probit_model, [0.0] * 4, args=(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exercise_\n",
    "\n",
    "* Amend the code so that you can simply switch between estimating a Probit or Logit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Resources\n",
    "\n",
    "\n",
    "* **Kevin T. Carlberg**: https://kevintcarlberg.net\n",
    "\n",
    "### Software \n",
    "\n",
    "* **Ipopt**: https://coin-or.github.io/Ipopt\n",
    "\n",
    "* **SNOPT (Sparse Nonlinear OPTimizer)**: https://ccom.ucsd.edu/~optimizers/solvers/snopt\n",
    "\n",
    "* **Gurobi** https://www.gurobi.com\n",
    "\n",
    "* **IBM CPLEX Optimizer** https://www.ibm.com/analytics/cplex-optimizer\n",
    "\n",
    "### Books\n",
    "\n",
    "* Nocedal, J., & Wright, S. (2006). [*Numerical optimization*](https://www.amazon.de/-/en/Jorge-Nocedal/dp/0387303030/ref=sr_1_1?dchild=1&keywords=nocedal+wright&qid=1605168584&sr=8-1) . Springer Science & Business Media.\n",
    "\n",
    "* Boyd, S., Boyd, S. P., & Vandenberghe, L. (2004). [*Convex optimization*](https://www.amazon.de/-/en/Stephen-Boyd/dp/0521833787/ref=sr_1_3?dchild=1&keywords=optimization&qid=1605168744&sr=8-3). Cambridge university press.\n",
    "\n",
    "* Kochenderfer, M. J., & Wheeler, T. A. (2019). [*Algorithms for optimization*](https://www.amazon.de/-/en/Mykel-J-Kochenderfer/dp/0262039427/ref=sr_1_1?dchild=1&keywords=optimization&qid=1605168758&sr=8-1). Mit Press.\n",
    "\n",
    "* Fletcher, R. (2000). [*Practical methods of optimization (2nd edn)*](https://www.amazon.com/gp/product/0471494631/ref=ox_sc_act_title_1?ie=UTF8&smid=ATVPDKIKX0DER#reader_0471494631). Wiley.\n",
    "\n",
    "* Nesterov, Y. (2018). [*Lectures on convex optimization*](https://www.springer.com/gp/book/9783319915777#aboutBook). Springer Nature Switzerland.\n",
    "\n",
    "### Research\n",
    "\n",
    "* Moré, J. J., & Wild, S. M. (2009). [Benchmarking derivative-free optimization algorithms](https://epubs.siam.org/doi/abs/10.1137/080724083?journalCode=sjope8). *SIAM Journal on Optimization*, 20(1), 172-191.\n",
    "\n",
    "* Beiranvand, V., Hare, W., & Lucet, Y. (2017). [Best practices for comparing optimization algorithms](https://link.springer.com/article/10.1007/s11081-017-9366-1). *Optimization and Engineering*, 18, 815–848.\n",
    "\n",
    "* Bartz-Beielstein, T., et al. (2020). [Benchmarking in optimization: Best practice and open issues](https://link.springer.com/article/10.1007/s11081-017-9366-1). *arXiv preprint arXiv:2007.03488.*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
